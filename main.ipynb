{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from llama_index.core import Document, PropertyGraphIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "from extractor import GraphRAGExtractor\n",
    "from query import GraphRAGQueryEngine\n",
    "from store import GraphRAGStore\n",
    "\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fn(response_str: str) -> Any:\n",
    "    entities = re.findall(entity_pattern, response_str)\n",
    "    relationships = re.findall(relationship_pattern, response_str)\n",
    "    return entities, relationships\n",
    "\n",
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "-Goal-\n",
    "Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities.\n",
    "Given the text, extract up to {max_knowledge_triplets} entity-relation triplets.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: Type of the entity\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "Format each entity as (\"entity\")\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relation: relationship between source_entity and target_entity\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "\n",
    "Format each relationship as (\"relationship\")\n",
    "\n",
    "3. When finished, output.\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "text: {text}\n",
    "######################\n",
    "output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Load sample dataset\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "# Load sample dataset\n",
    "print(\"Step 1: Load sample dataset\")\n",
    "news = pd.read_csv(\"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\")[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Convert data into LlamaIndex Document objects\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "# Convert data into LlamaIndex Document objects\n",
    "print(\"Step 2: Convert data into LlamaIndex Document objects\")\n",
    "documents = [\n",
    "    Document(text=f\"{row['title']}: {row['text']}\")\n",
    "    for _, row in news.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Split documents into nodes\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "print(\"Step 3: Split documents into nodes\")\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Initialize LLM\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "print(\"Step 4: Initialize LLM\")\n",
    "OpenAI.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OpenAI.api_key)\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "entity_pattern = r'entity_name:\\s*(.+?)\\s*entity_type:\\s*(.+?)\\s*entity_description:\\s*(.+?)\\s*'\n",
    "relationship_pattern = r'source_entity:\\s*(.+?)\\s*target_entity:\\s*(.+?)\\s*relation:\\s*(.+?)\\s*relationship_description:\\s*(.+?)\\s*'\n",
    "\n",
    "kg_extractor = GraphRAGExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "    max_paths_per_chunk=2,\n",
    "    parse_fn=parse_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Create PropertyGraphIndex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:22<00:00, 22.66s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 5\n",
    "print(\"Step 5: Create PropertyGraphIndex\")\n",
    "index = PropertyGraphIndex(\n",
    "    nodes=nodes,\n",
    "    property_graph_store=GraphRAGStore(),\n",
    "    kg_extractors=[kg_extractor],\n",
    "    show_progress=True,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Build communities\n"
     ]
    },
    {
     "ename": "EmptyNetworkError",
     "evalue": "EmptyNetworkError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyNetworkError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 6\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 6: Build communities\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperty_graph_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_communities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/RAG/store.py:37\u001b[0m, in \u001b[0;36mGraphRAGStore.build_communities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Builds communities from the graph and summarizes them.\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m nx_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_nx_graph()\n\u001b[0;32m---> 37\u001b[0m community_hierarchical_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mhierarchical_leiden\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnx_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_cluster_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_cluster_size\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m community_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_community_info(\n\u001b[1;32m     41\u001b[0m     nx_graph, community_hierarchical_clusters\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_communities(community_info)\n",
      "File \u001b[0;32m<@beartype(graspologic.partition.leiden.hierarchical_leiden) at 0x328862020>:304\u001b[0m, in \u001b[0;36mhierarchical_leiden\u001b[0;34m(__beartype_object_13563157440, __beartype_object_13557955136, __beartype_getrandbits, __beartype_get_violation, __beartype_conf, __beartype_object_4332659400, __beartype_object_13436738688, __beartype_object_13557955072, __beartype_object_13565503184, __beartype_object_13565502208, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/graspologic/partition/leiden.py:588\u001b[0m, in \u001b[0;36mhierarchical_leiden\u001b[0;34m(graph, max_cluster_size, starting_communities, extra_forced_iterations, resolution, randomness, use_modularity, random_seed, weight_attribute, is_weighted, weight_default, check_directed)\u001b[0m\n\u001b[1;32m    580\u001b[0m     node_count, edges \u001b[38;5;241m=\u001b[39m _adjacency_matrix_to_edge_list(\n\u001b[1;32m    581\u001b[0m         graph, identifier, check_directed, is_weighted, weight_default\n\u001b[1;32m    582\u001b[0m     )\n\u001b[1;32m    584\u001b[0m native_friendly_communities \u001b[38;5;241m=\u001b[39m _community_python_to_native(\n\u001b[1;32m    585\u001b[0m     starting_communities, identifier\n\u001b[1;32m    586\u001b[0m )\n\u001b[0;32m--> 588\u001b[0m hierarchical_clusters_native \u001b[38;5;241m=\u001b[39m \u001b[43mgn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhierarchical_leiden\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstarting_communities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnative_friendly_communities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_forced_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_modularity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_modularity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_cluster_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_cluster_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m result_partitions \u001b[38;5;241m=\u001b[39m HierarchicalClusters()\n\u001b[1;32m    600\u001b[0m all_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mEmptyNetworkError\u001b[0m: EmptyNetworkError"
     ]
    }
   ],
   "source": [
    "# Step 6\n",
    "print(\"Step 6: Build communities\")\n",
    "index.property_graph_store.build_communities() # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7\n",
    "print(\"Step 7: Query the graph\")\n",
    "query_engine = GraphRAGQueryEngine(\n",
    "    graph_store=index.property_graph_store, llm=llm\n",
    ")\n",
    "response = query_engine.query(\"What are news related to financial sector?\")\n",
    "print(Markdown(f\"{response.response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
